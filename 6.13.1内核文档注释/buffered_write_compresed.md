## 功能背景（就不和下面的重大问题分析撞车了)
我们已经决定将整个buffered write路径迁移往iomap了,因为经过我们前面的分析我们知道这是最无包袱的支持large folios的方式。而和buffered read和page writeback
不同的一点是,iomap_buffered_write除了提供iomap_begin和iomap_end,还提供了我们可以自己定制化获取folio逻辑和释放folio逻辑的__iomap_get_folio和__iomap_put_folio。通过
分析,我发现用iomap buffered write实现压缩文件的buffered write是可能的并且函数栈的调用路径可以做到更少的调用栈开销。
## 重大问题剖析
初赛时候我们的设计方案
是我们将改造成支持large folios的prepare write begin逻辑 
也即原先的buffered write的逻辑 给拿过来填入到iomap_begin之中。然后__iomap_get_folio之中我们和原代码逻辑一样,从压缩文件上下文中根据我们的索引取出里面的page转化为folio
结构体。但是因为iomap_get_folio的策略和我们在iomap_folio_state中存储的length是只被限制在一个簇之中 这里我们要贴出iomap_get_folio中传入阶数hint的地方了。
```C
iomap_get_folio
```
所以我发现我们几乎是严重限制了large folios针对buffered write的性能 因为我们大大地限制了其能分配的folio的上限

## 方案设计
我们切换了视角 不再是以cluster的视角出发 一点一点用folio将cluster给填满 转而切换为从一个大folio的视角出发 不断地用cluster去填充它特别强调这段是我们自己的原创算法
然后我们将整个处理好的folio就保持着上锁的状态 然后将其放到iomap->private之中 送入iomap的主要处理逻辑。注意在我们iomap_begin中这样处理的逻辑 最终总是会让我们拿到的这个folio全为uptodate
这里面还有个非常重要的点。f2fs内提交io的代码全部都是异步的。但是我们这个地方和所有的 iomap buffered write一样是必须等到整个folio全部给读完 直到全部uptodate 我们才能对其进行写入，
不然就会出现还没等folio内容全部从磁盘读上来,就被buffered write写入造成的严重数据不一问题。而我们面临的问题是f2fs原生的代码并没直接提供给我们这样进行同步等待的设施,而内核虽然具有submit_bio_wait,但是它只能做到等待一个bio级别部分被完成,没法做到等待folio的全部部分。于是,这个时候我想到有一种很好的利用`read_bytes_pending`的主意,首先我们先加上一个bias,这样我们就能确保所有的bio回调不会被调用(因为始终有这个bias阻止它们调用)，然后我们显式地在iomap_begin中轮询直到read_bytes_pending的值刚好是f2fs_ifs_magic+1.此时folio必然是全uptodate状态。同时为了避免出现有一个压缩簇里只有部分folio处于uptodate这种尴尬的情况,（尤其是这个folio本身就不够覆盖这个簇的情况下)我还显式地填满了这个folio最后一个所在的簇。
